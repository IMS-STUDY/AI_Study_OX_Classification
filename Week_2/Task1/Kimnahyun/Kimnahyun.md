# ì¸ê³µì§€ëŠ¥ 2-1ê³¼ì œ

2ì£¼-1

ì£¼í”¼í„°ì— ì½”ë“œ ëˆ„ì 

- [x]  OX_Classification
- MLPë¡œ ë¶„ë¥˜ ì§„í–‰í•˜ê¸°
    - ~~OX_Dataset ë¶ˆëŸ¬ì˜¤ê¸°~~
    - ~~OX_image ì „ì²˜ë¦¬~~
    - MLPë¡œ ì§„í–‰ì‹œ k-foldì ìš© , ë¯¸ì ìš© ê°ê° ì •í™•ë„ ì‚°ì¶œ
- ~~acitvation function~~
    - ~~acitvation functionë€?~~
        - ~~ëª©ì ~~
    - ~~ì¢…ë¥˜, íŠ¹ì§•~~
        - ~~sigmoid~~
        - ~~tanh~~
        - ~~relu~~
        - ~~softmax~~
- ~~k-fold~~
    - ~~k-foldë€?~~
    - ~~k-fold ì›ë¦¬~~
    

## 1. MLPë¡œ ë¶„ë¥˜ ì§„í–‰í•˜ê¸°

### 1) OX_dataset ë¶ˆëŸ¬ì˜¤ê¸°

```python
from __future__ import unicode_literals, print_function, division
from io import open
import glob
import os

path = r"C:\Users\USER\Desktop\Dataset"
def findFiles(path): return glob.glob(path)
#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
```

â¡ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê³¼ì •ì´ë‹¤. ë°›ì€ ë°ì´í„° íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ì£¼ì†Œ(ì ˆëŒ€ê²½ë¡œ)ë¡œ ì ‘ê·¼ í•˜ì˜€ìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python

#í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •
training_epochs = 10
batch_size = 32

learning_rate = 0.001

input_size = 90000    # 300x300 image       # ê³ ì •ëœ ê°’ (ì´ë¯¸ì§€ í¬ê¸°)
hidden_size = 1000   # ì„ì˜ì˜ ê°’           # ì„ì˜ì˜ ê°’ (hidden layerì˜ ë…¸ë“œ ê°œìˆ˜)
output_size = 2    # OX               # ê³ ì •ëœ ê°’ (ë¶„ë¥˜í•  í´ë˜ìŠ¤ ê°œìˆ˜)
```

â¡ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ëŠ” ì½”ë“œì´ë‹¤. 

ê¸°ë³¸ì ì¸ epochì™€ batch size, ê·¸ë‹¤ìŒ input sizeì™€ hidden size, output sizeë¥¼ ì •ì˜í•´ì£¼ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤.

### 2) OX_image ì „ì²˜ë¦¬

```python
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms

image_list1 = [os.path.join(path, fname) for fname in os.listdir(path) if fname.endswith('.png')]
image_list2 = [os.path.join(path, fname) for fname in os.listdir(path) if fname.endswith('.jpg')]

image_list = image_list1+image_list2
transform = transforms.ToTensor()

#image = Image.open(image_list[0]).convert("L")
#image = transform(image)
dataset = [transform(Image.open(idx).convert("L")) for idx in image_list]

print(f"dataset ë°°ì—´ í¬ê¸° : {len(dataset)}")
print(f"ì²«ë²ˆì§¸ ì›ì†Œê°’ ì‚¬ì´ì¦ˆ: {dataset[0].size()}")
```

```
dataset ë°°ì—´ í¬ê¸° : 280
ì²«ë²ˆì§¸ ì›ì†Œê°’ ì‚¬ì´ì¦ˆ: torch.Size([1, 300, 300])
```

â¡ pathì— ìˆëŠ”(dataset) pngíŒŒì¼ê³¼ jpgíŒŒì¼ì„ êº¼ë‚´ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê³  ê°ê°ì˜ ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ image_listì— í•©ì¹œë‹¤. ê·¸ ë‹¤ìŒ transforms.ToTensor()ë¥¼ í†µí•´ image_listì˜ ëª¨ë“  ë°ì´í„°ë¥¼  **"Channel x Height x Width" êµ¬ì¡°**ë¡œ ë°”ê¿”ì¤€ë‹¤. 

â¡ ë°°ì—´ í¬ê¸°ëŠ” 280ê°œ (O : 140ê°œ, X : 140ê°œ)ê°€ ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.  

<aside>
ğŸ’¡ **ì´í›„ ì‚¬ì´ì¦ˆë¥¼ ë°”ê¾¸ëŠ” ê³¼ì •ê³¼ ë¼ë²¨ë§ì„ ì¶”ê°€í•¨**

</aside>

```python

from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms

import cv2
import numpy as np
from matplotlib import pyplot as plt

image_list1 = [os.path.join(path, fname) for fname in os.listdir(path) if fname.endswith('.png')]
image_list2 = [os.path.join(path, fname) for fname in os.listdir(path) if fname.endswith('.jpg')]

image_list = image_list1+image_list2
transform = transforms.ToTensor()

#image = Image.open(image_list[0]).convert("L")
#image = transform(image)

dataset = []
labels = []

for idx in image_list:
    image = cv2.imread(idx, cv2.IMREAD_GRAYSCALE)
    image = cv2.resize(image, (50,50))
    image = Image.fromarray(image)
    image = transform(image)
    dataset.append(image)

    filename = os.path.basename(idx).lower()
    if 'o' in filename:
        labels.append(0) #ê²°ê³¼ ë ˆì´ë¸” 'o'
    elif 'x' in filename:
        labels.append(1) #ê²°ê³¼ ë ˆì´ë¸” 'x'

print(f"dataset ë°°ì—´ í¬ê¸° : {len(dataset)}")
print(f"ì²«ë²ˆì§¸ ì›ì†Œê°’ ì‚¬ì´ì¦ˆ: {dataset[0].size()}")
```

```
dataset ë°°ì—´ í¬ê¸° : 280
ì²«ë²ˆì§¸ ì›ì†Œê°’ ì‚¬ì´ì¦ˆ: torch.Size([1, 50, 50])
```

â¡ ì‚¬ì´ì¦ˆ í¬ê¸°ë¥¼ 50*50 í”½ì…€ì˜ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì˜€ë‹¤. ê·¸ë¦¬ê³  ì´ë¯¸ì§€ì˜ íŒŒì¼ëª…ì„ í†µí•´ì„œ ë°ì´í„°ì˜ ë¼ë²¨ë§ ë°°ì—´ì„ ë§Œë“¤ì–´ì£¼ì—ˆë‹¤.

```python
# ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
labels = torch.tensor(labels, dtype=torch.long)

train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size

train_dataset, test_dataset = random_split(list(zip(dataset, labels)), [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size, shuffle=False
```

â¡ ê·¸ë‹¤ìŒ 8:2 ë¹„ìœ¨ë¡œ train ë°ì´í„°ì™€ test ë°ì´í„°ë¥¼ ë‚˜ëˆ  ë‹´ëŠ”ë‹¤. ê·¸ë¦¬ê³  `DataLoader`ë¥¼ ì´ìš©í•´ train ë°ì´í„°ì™€ test ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. batch sizeëŠ” ìœ„ì—ì„œ ì •ì˜í•œ 32ì´ë¼ëŠ” ê°’ì´ë‹¤.

### 3) MLPë¡œ ì§„í–‰ì‹œ k-foldì ìš© , ë¯¸ì ìš© ê°ê° ì •í™•ë„ ì‚°ì¶œ

- ë¯¸ì ìš©ì˜ ê²½ìš°

```python
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.sequential = nn.Sequential(
            nn.Flatten(),  # 50*50 ì´ë¯¸ì§€ë¥¼ 2500*1 ë²¡í„°ë¡œ ë³€í™˜
            nn.Linear(in_features=2500, out_features=1250, bias=True),
            nn.ReLU(),
            nn.Linear(in_features=1250, out_features=625, bias=True),
            nn.ReLU(),
            nn.Linear(in_features=625, out_features=315, bias=True),
            nn.ReLU(),
            nn.Linear(in_features=315, out_features=150, bias=True),
            nn.ReLU(),
            nn.Linear(in_features=150, out_features=70, bias=True),
            nn.ReLU(),
            nn.Linear(in_features=70, out_features=2, bias=True)
        )

    def forward(self, x):
        return self.sequential(x)

model = MLP()
criterion = nn.CrossEntropyLoss() #ì†ì‹¤í•¨ìˆ˜
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #ì˜µí‹°ë§ˆì´ì €
#learning rate 0.1 -> 0.0001ë¡œ ë³€ê²½
```

â¡  MLP ëª¨ë¸ì„ ë§Œë“¤ì–´ì£¼ê³  ì†ì‹¤í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì •ì˜ í•´ì¤€ë‹¤. ì˜µí‹°ë§ˆì´ì €ëŠ” ìœ ëª…í•œ Adamì„ ì‚¬ìš©í–ˆê³ , ì†ì‹¤í•¨ìˆ˜ë„ 2ì§„ ë¶„ë¥˜ì— ì í•©í•œ CrossEntropyë¥¼ ì‚¬ìš©í–ˆë‹¤.

- ëª¨ë¸ í•™ìŠµ

```python
for epoch in range(100): #10->100ìœ¼ë¡œ ì¦ê°€
    avg_cost = 0
    total_batch = len(train_loader)
    
    for images, labels in train_loader:
        optimizer.zero_grad()

        hypothesis = model(images)

        cost = criterion(hypothesis, labels)
        cost.backward()
        
        optimizer.step()
        avg_cost += cost / total_batch

    print("Epoch:", "%04d" % (epoch + 1), "cost =", "{:.9f}".format(avg_cost))

```

```
Epoch: 0001 cost = 0.693798721
Epoch: 0002 cost = 0.692188799
Epoch: 0003 cost = 0.691086352
Epoch: 0004 cost = 0.689724028
Epoch: 0005 cost = 0.686602116
Epoch: 0006 cost = 0.691653848
Epoch: 0007 cost = 0.683707118
Epoch: 0008 cost = 0.679791152
Epoch: 0009 cost = 0.676595092
Epoch: 0010 cost = 0.671693802
...
Epoch: 0097 cost = 0.148798212
Epoch: 0098 cost = 0.160334587
Epoch: 0099 cost = 0.080024049
Epoch: 0100 cost = 0.068397321
```

â¡ ê·¸ ë‹¤ìŒ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•œë‹¤. epochê°’ì€ ê¸°ì¡´ì— 10ìœ¼ë¡œ ì„¤ì •í•´ë†¨ì§€ë§Œ 100ìœ¼ë¡œ ëŠ˜ë ¤ë†¨ìœ¼ë©°, ìœ„ì— ì˜µí‹°ë§ˆì´ì €ì˜ learning rateë„ 0.0001 ë³€ê²½í–ˆë‹¤. 

printë¡œ ì¶œë ¥ì„ í•´ë³´ë©´ cost(ì†ì‹¤)ì´ ê¾¸ì¤€íˆ ì¤„ì–´ë“œëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ëª¨ë¸ í‰ê°€

```python
import random
import matplotlib.pyplot as plt

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€
model.eval()
correct = 0
total = 0
with torch.no_grad():  # torch.no_grad()ë¥¼ í•˜ë©´ gradient ê³„ì‚°ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f'Accuracy of the model on the test images: {accuracy:.2f}%')

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ë¥¼ ë½‘ì•„ì„œ ì˜ˆì¸¡
r = random.randint(0, len(test_dataset) - 1)
single_image, single_label = test_dataset[r]
single_image = single_image.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€
with torch.no_grad():
    single_prediction = model(single_image)
    predicted_label = torch.argmax(single_prediction, 1).item()

print(f'Label: {single_label.item()}')
print(f'Prediction: {predicted_label}')

# ì´ë¯¸ì§€ ì¶œë ¥
plt.imshow(single_image.squeeze().numpy(), cmap="Greys", interpolation="nearest")
plt.show()
```

```python
Accuracy of the model on the test images: 75.00%
Label: 1
Prediction: 1
```

![Untitled](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/6c74f396-c56b-4472-835c-189ce2eec6ae)

â¡ï¸ ìµœì¢…ì ìœ¼ë¡œ 75% ì •ë„ì˜ ì •í™•ë„ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 

---

## 2. Activation function

### 1) activation functionë€?

= í™œì„±í™” í•¨ìˆ˜ë¼ëŠ” ëœ»ìœ¼ë¡œ, ë…¸ë“œì˜ ì¶œë ¥ì„ ì–»ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ë‹¨ìˆœí•œ ì‚¬ë¬¼ í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ì „ë‹¬í•¨ìˆ˜ì˜ ì˜ë¯¸ë¡œ ì‚¬ìš©ë˜ê¸°ë„ í•œë‹¤.

- í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ (ëª©ì )

= ì‹ ê²½ë§ì˜ ì¶œë ¥ì„ ì˜ˆ/ì•„ë‹ˆì˜¤ë¡œ ê²°ì •í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. í˜¹ì€ ê²°ê³¼ê°’ì„ 0~1ë˜ëŠ” -1~1 ë“±ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ì´ëŠ” í•¨ìˆ˜ì— ë”°ë¼ ê²°ê³¼ê°’ì´ ë‹¤ë¥´ì§€ë§Œ ê²°ë¡ ì ìœ¼ë¡œëŠ” ì¶œë ¥ì„ ì œì–´í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤.

### 2) ì¢…ë¥˜, íŠ¹ì§•

í™œì„±í™” í•¨ìˆ˜ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 2ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. 

â†’ **ì„ í˜• í™œì„±í™” í•¨ìˆ˜**

![Untitled 1](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/085ff007-b284-4f21-9163-a828b9fb99ac)

ë°©ì •ì‹ :  f(x) = x

ë²”ìœ„ : ( -âˆ, +âˆ )

íŠ¹ì§• : í•´ë‹¹ í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” ì‹ ê²½ë§ì— ì…ë ¥ë˜ëŠ” ë³µì¡í•œ ë°ì´í„°ë‚˜ ë‹¤ì–‘í•œ ë§¤ê°œë³€ìˆ˜ ì²˜ë¦¬ëŠ” í˜ë“¤ë‹¤.  

â†’ **ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜**

![Untitled 2](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/eaff28e9-91b0-40a0-b4ee-4b27a0b4586b)

: ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ëŠ” ì—°ì†ì ì´ê³  ë¶ˆê·œì¹™í•œ ë°ì´í„°ì— ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë°ì´í„°ì— ë§ê²Œ ëª¨ë¸ì„ ì¼ë°˜í™”í•˜ê±°ë‚˜ ì ì‘í•˜ê³  ì¶œë ¥ì„ êµ¬ë³„í•˜ëŠ” ê²ƒì´ ì‰¬ì›Œì§„ë‹¤.

ì•„ë˜ì— ì´ì–´ì§ˆ í™œì„±í™” í•¨ìˆ˜ë„ ì´ëŸ¬í•œ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë“¤ì´ë‹¤.

- **sigmoid**

![Untitled 3](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/acc83387-8269-4a2f-873e-1298e8b333e1)

: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ê³¡ì„ ì€ Sì ëª¨ì–‘ì´ë‹¤. 

```python

# sigmoid í™œì„±í™” í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# sigmoid í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜
def sigmoid_derivative(x):
    sigmoid_x = 1 / (1 + np.exp(-x))
    derivative = sigmoid_x * (1 - sigmoid_x)
    return derivative
```

ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì£¼ëœ ì´ìœ ëŠ” **0~1 ì‚¬ì´ì— ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤.** ë”°ë¼ì„œ ì¶œë ¥ì„ í™•ë¥ ë¡œ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ìì£¼ ì‚¬ìš©ë˜ë©°, **í™•ë¥ ì€ 0~1ì˜ ë²”ìœ„ ì‚¬ì´ì— ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— ì‹œê·¸ëª¨ì´ë“œê°€ ì í•©**í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

- tanh

![Untitled 4](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/183e8d4c-9e9d-4570-bdba-72e3de9621d3)

: tanhëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ë²”ìœ„ì—ì„œ ì°¨ì´ì ì´ ìˆë‹¤. 

tanh í•¨ìˆ˜ì˜ ì¶œë ¥ ë²”ìœ„ëŠ” (-1, 1)ë¡œ, ì‹œê·¸ëª¨ì´ë“œì²˜ëŸ¼ Sì ê³¡ì„  í˜•íƒœë©° ìŒìˆ˜ ì…ë ¥ì´ ê°•í•˜ê²Œ ìŒìˆ˜ë¡œ ë§¤í•‘ë˜ê³  0 ì…ë ¥ì€ 0ì— ê°€ê¹ê²Œ ë§¤í•‘í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. 

```python
# tanh í™œì„±í™” í•¨ìˆ˜
def tanh(x):
    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

# tanh í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜
def tanh_derivative(x):
    tanh_x = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
    derivative = 1 - tanh_x**2
    return derivative
```

tanh í•¨ìˆ˜ëŠ” ì£¼ë¡œ ë‘ í´ë˜ìŠ¤ ê°„ì˜ ë¶„ë¥˜ì— ì‚¬ìš©ëœë‹¤.

- relu

ReLUëŠ” í˜„ì¬ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ë¡œ, ê±°ì˜ ëª¨ë“  í•©ì„±ê³± ì‹ ê²½ë§ì´ë‚˜ ë”¥ëŸ¬ë‹ì— ì‚¬ìš©ëœë‹¤. 

![Untitled 5](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/c5f0bb8c-19c6-40d8-83f9-a426d06150eb)

Sigmoid vs ReLU

ReLU í•¨ìˆ˜ëŠ” 0ì´í•˜ì˜ ìŒìˆ˜ê°’ì€ ì „ë¶€ 0ìœ¼ë¡œ ì²˜ë¦¬ë˜ê³  ì–‘ìˆ˜ê°’ì€ ê·¸ëŒ€ë¡œ ì¶œë ¥ë˜ëŠ” íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆë‹¤.
â†’ ë²”ìœ„ : (0 , +âˆ )

```python
# ReLU í™œì„±í™” í•¨ìˆ˜
def relu(X):
    return np.maximum(0, X)

# ReLU í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜
def relu_derivative(X):
    return np.where(X > 0, 1, 0)
```

í•˜ì§€ë§Œ ëª¨ë“  ìŒìˆ˜ê°’ì´ ì¦‰ì‹œ 0ì´ ë˜ì–´ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì ì ˆíˆ ë§ì¶”ê±°ë‚˜ í•™ìŠµí•˜ëŠ” ëŠ¥ë ¥ì€ ê°ì†Œí•œë‹¤. ì¦‰, ReLU í™œì„±í™” í•¨ìˆ˜ì— ì£¼ì–´ì§„ ìŒìˆ˜ ì…ë ¥ì€ ê·¸ë˜í”„ì—ì„œ ê°’ì„ ì¦‰ì‹œ 0ìœ¼ë¡œ ë°”ê¾¸ê³ , ì ì ˆí•œ ë§¤í•‘ì´ ì´ë£¨ì–´ì§€ì§€ì•Šì•„ ê²°ê³¼ ê·¸ë˜í”„ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

- softmax

= softmax í•¨ìˆ˜ëŠ” í™•ë¥ ì²˜ëŸ¼ ëª¨ë“  ì¶œë ¥ ê°’ì„ ë”í–ˆì„ ë•Œ 1ì´ ì´í•©ì´ë¼ëŠ” íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆë‹¤.

softmax í•¨ìˆ˜ì— ì…ë ¥ê°’ì„ ë„£ìœ¼ë©´, ê·¸ ê°’ë“¤ì„ ëª¨ë‘ 0ê³¼ 1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì •ê·œí™”í•´ì£¼ëŠ”ë°, ì´ëŠ” ê° í™•ë¥ ì´ ë§ˆì´ë„ˆìŠ¤ ê°’ì„ ê°€ì§€ì§€ì•Šê³  ë”í–ˆì„ ë•Œ ì´í•©ì´ 1ì´ ë˜ëŠ” ê²ƒê³¼ ë§¤ìš° í¡ì‚¬í•˜ë‹¤.

â†’ softmaxí•¨ìˆ˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì—ì„œ ì…ë ¥ê°’ì´ nê°œë¡œ ëŠ˜ì–´ë‚œ ê²ƒê³¼ ê°™ë‹¤.

```python
# softmax í™œì„±í™” í•¨ìˆ˜
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / np.sum(e_x, axis=0)

# softmax í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜
def softmax_derivative(X):
    e_x = np.exp(x - np.max(x))
    softmax_x = e_x / np.sum(e_x, axis=0)
    derivative = softmax_x * (1 - softmax_x)
    return derivative
```

: ì •ë¦¬í•˜ìë©´ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ì…ë ¥ê°’ì´ í•˜ë‚˜ì¼ ë•Œ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ê³ , ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì…ë ¥ê°’ì´ ì—¬ëŸ¬ê°œì¼ ë•Œë„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ì¼ë°˜í™”í•œ ê²ƒì´ softmaxí•¨ìˆ˜ì´ë‹¤. 

> https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
> 

> https://financial-engineering.medium.com/ml-softmax-ì†Œí”„íŠ¸ë§¥ìŠ¤-í•¨ìˆ˜-2f4740141bfe
> 

---

## 3. k-fold

<aside>
ğŸ’¡ ë‚˜ì™€ì„â€¦.ë§ˆ? ë§ˆ! ë‚˜ ì™€ ì„

</aside>

### 1) k-foldë€?

= **k-fold cross validation**ë€ ****í•™ìŠµìš©/í‰ê°€ìš© ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ” ë°©ë²•ë¡  ì¤‘ í•˜ë‚˜ë¥¼ ì˜ë¯¸í•œë‹¤. 

ë¨¼ì € êµì°¨ ê²€ì¦ì— ëŒ€í•´ ë¨¼ì € ì•Œì•„ë³´ìë©´ ì‰½ê²Œ ë§í•´ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ì„œ ë‚˜ëˆ„ê³  ì—¬ëŸ¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì´ë‹¤. 

ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” ë°ì´í„°ë¥¼ í•™ìŠµìš©/í‰ê°€ìš© ë°ì´í„° ì„¸íŠ¸ë¡œ ì—¬ëŸ¬ë²ˆ ë‚˜ëˆˆ ê²ƒì˜ í‰ê· ì ì¸ ì„±ëŠ¥ì„ ê³„ì‚°í•˜ë©´ í•œ ë²ˆ ë‚˜ëˆ„ì–´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì— ë¹„í•´ ì¼ë°˜í™”ëœ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. 

![cv](https://github.com/IMS-STUDY/2024_Summer_AI_Study/assets/127017020/66228adf-0112-4788-9040-f8114c8df787)

:  k-fold êµì°¨ ê²€ì¦ì€ ë°ì´í„°ë¥¼ kê°œë¡œ ë¶„í• í•œ ë’¤,  k-1ê°œë¥¼ í•™ìŠµìš© ë°ì´í„° ì„¸íŠ¸ë¡œ, 1ê°œë¥¼ í‰ê°€ìš© ë°ì´í„° ì„¸íŠ¸ë¡œ ì‚¬ìš©í•˜ëŠ”ë° ì´ë¥¼ kë²ˆ ë°˜ë³µí•˜ì—¬ kê°œì˜ ì„±ëŠ¥ ì§€í‘œë¥¼ ì–»ì–´ë‚´ëŠ” ë°©ë²•ì´ë‹¤.

### 2) k-fold ì›ë¦¬

: k-foldëŠ” ë³´í†µ 5ë˜ëŠ” 10ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ ê°’ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤.  

â¡ï¸ k=5ì¸ ê²½ìš°ë¥¼ ì˜ˆë¡œ ë“¤ìë©´, ë°ì´í„°ë¥¼ í´ë“œ(fold)ë¼ê³  í•˜ëŠ” ë¹„ìŠ·í•œ í¬ê¸°ì˜ ë¶€ë¶„ ì§‘í•© 5ê°œë¡œ ë‚˜ëˆˆë‹¤. 

ì²« ë²ˆì§¸ ëª¨ë¸ì€ ì²« ë²ˆì§¸ foldë¥¼ í‰ê°€ìš© ë°ì´í„° ì…‹ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ë‘ ë²ˆì§¸ë¶€í„° ë‹¤ì„¯ ë²ˆì§¸ê¹Œì§€ì˜ í´ë“œ(4ê°œì˜ í´ë“œ)ë¥¼ í•™ìŠµìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

ê·¸ ë‹¤ìŒ ëª¨ë¸ì€ ë‘ ë²ˆì§¸ í´ë“œë¥¼ í‰ê°€ìš©ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , 1, 3, 4, 5 í´ë“œë¥¼ í•™ìŠµìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ì´í›„ ë‚˜ë¨¸ì§€ë„ ê°™ì€ ê³¼ì •ì„ ì§„í–‰í•˜ì—¬ ì´ 5ê°œì˜ ì •í™•ë„ ê°’ì„ ì–»ëŠ”ë‹¤.

- Python êµ¬í˜„ ë°©ë²•

: k-fold cross-validationì„ pythonì„ í†µí•´ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì. ë°ì´í„°ëŠ” iris ë°ì´í„°ë¥¼, ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œëŠ” scikit-learnì„ í™œìš©í•œë‹¤. ë‹¤ì–‘í•œ ë°©ë²•ì„ í†µí•´ cross-validationì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ”Â `sklearn.model_selection.cross_val_score`ë¥¼ í™œìš©í•œë‹¤.

```python
# iris ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
iris= load_iris()

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
logreg = LogisticRegression()

# k=5ë¡œ k-fold cross validation ìˆ˜í–‰
scores = cross_val_score(logreg, iris.data, iris.target, cv=5)
print("êµì°¨ ê²€ì¦ ì ìˆ˜: ", scores)
```

> https://incodom.kr/k-ê²¹_êµì°¨_ê²€ì¦
> 

---